# Installing on OpenShift Container Platform

Thank you for your interest in Red Hat Ansible Automation Platform. Ansible Automation Platform is a commercial offering that helps teams manage complex multi-tier deployments by adding control, knowledge, and delegation to Ansible-powered environments.
This guide helps you to understand the installation, migration and upgrade requirements for deploying the Ansible Automation Platform Operator on OpenShift Container Platform.

# Providing feedback on Red Hat documentation

If you have a suggestion to improve this documentation, or find an error, you can contact technical support at https://access.redhat.com to open a request.

# Planning your Red Hat Ansible Automation Platform Operator on Red Hat OpenShift Container Platform

Red Hat Ansible Automation Platform is supported on both Red Hat Enterprise Linux and Red Hat Openshift.

OpenShift operators help install and automate day-2 operations of complex, distributed software on Red Hat OpenShift Container Platform. The Ansible Automation Platform Operator enables you to deploy and manage Ansible Automation Platform components on Red Hat OpenShift Container Platform.

You can use this section to help plan your Red Hat Ansible Automation Platform installation on your Red Hat OpenShift Container Platform environment. Before installing, review the supported installation scenarios to determine which meets your requirements.

## About Ansible Automation Platform Operator

The Ansible Automation Platform Operator provides cloud-native, push-button deployment of new Ansible Automation Platform instances in your OpenShift environment.
The Ansible Automation Platform Operator includes resource types to deploy and manage instances of automation controller and private automation hub.
It also includes automation controller job resources for defining and launching jobs inside your automation controller deployments.

Deploying Ansible Automation Platform instances with a Kubernetes native operator offers several advantages over launching instances from a playbook deployed on Red Hat OpenShift Container Platform, including upgrades and full lifecycle support for your Red Hat Ansible Automation Platform deployments.

You can install the Ansible Automation Platform Operator from the Red Hat Operators catalog in OperatorHub.

For information about the Ansible Automation Platform Operator infrastructure topology see Container topologies in Tested deployment models.

## OpenShift Container Platform version compatibility

The Ansible Automation Platform Operator to install Ansible Automation Platform 2.5 is available on OpenShift Container Platform 4.12 through to 4.17 and later versions.

* See the Red Hat Ansible Automation Platform Life Cycle for the most current compatibility details.

## Supported installation scenarios for Red Hat OpenShift Container Platform

You can use the OperatorHub on the Red Hat OpenShift Container Platform web console to install Ansible Automation Platform Operator.

Alternatively, you can install Ansible Automation Platform Operator from the OpenShift Container Platform command-line interface (CLI), oc. See Installing Red Hat Ansible Automation Platform Operator from the OpenShift Container Platform CLI for help with this.

After you have installed Ansible Automation Platform Operator you must create an Ansible Automation Platform custom resource (CR). This enables you to manage Ansible Automation Platform components from a single unified interface known as the platform gateway. As of version 2.5, you must create an Ansible Automation Platform CR, even if you have an existing automation controller,  automation hub, or Event-Driven Ansible, components.

If existing components have already been deployed, you must specify these components on the Ansible Automation Platform CR. You must create the custom resource in the same namespace as the existing components.



## Custom resources

You can define custom resources for each primary installation workflows.

### Modifying the number of simultaneous rulebook activations during or after Event-Driven Ansible controller installation

* If you plan to install Event-Driven Ansible on OpenShift Container Platform and modify the number of simultaneous rulebook activations, add the required EDA_MAX_RUNNING_ACTIVATIONS parameter to your custom resources. By default, Event-Driven Ansible controller allows 12 activations per node to run simultaneously. See the example in appendix EDA_MAX_RUNNING_ACTIVATIONS.


[NOTE]
----
EDA_MAX_RUNNING_ACTIVATIONS for OpenShift Container Platform is a global value since there is no concept of worker nodes when installing Event-Driven Ansible on OpenShift Container Platform.
----

## Additional resources

* See Understanding OperatorHub to learn more about OpenShift Container Platform OperatorHub.

# Installing the Red Hat Ansible Automation Platform Operator on Red Hat OpenShift Container Platform

* You have installed the Red Hat Ansible Automation Platform catalog in OperatorHub.
* You have created a StorageClass object for your platform and a persistent volume claim (PVC) with ReadWriteMany access mode. See Dynamic provisioning for details.
* To run Red Hat OpenShift Container Platform clusters on Amazon Web Services (AWS) with ReadWriteMany access mode, you must add NFS or other storage.
* For information about the AWS Elastic Block Store (EBS) or to use the aws-ebs storage class, see Persistent storage using AWS Elastic Block Store.
* To use multi-attach ReadWriteMany access mode for AWS EBS, see Attaching a volume to multiple instances with Amazon EBS Multi-Attach.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Search for Ansible Automation Platform and click Install.
4. Select an Update Channel:
* stable-2.x: installs a namespace-scoped operator, which limits deployments of automation hub and automation controller instances to the namespace the operator is installed in. This is suitable for most cases. The stable-2.x channel does not require administrator privileges and utilizes fewer resources because it only monitors a single namespace.
* stable-2.x-cluster-scoped: deploys automation hub and automation controller across multiple namespaces in the cluster and requires administrator privileges for all namespaces in the cluster.
5. Select Installation Mode, Installed Namespace, and Approval Strategy.
6. Click Install.

The installation process begins. When installation finishes, a modal appears notifying you that the Ansible Automation Platform Operator is installed in the specified namespace.

* Click View Operator to view your newly installed Ansible Automation Platform Operator.


[IMPORTANT]
----
You can only install a single instance of the Ansible Automation Platform Operator into a single namespace.
Installing multiple instances in the same namespace can lead to improper operation for both operator instances.
----

# Updating Ansible Automation Platform on OpenShift Container Platform

You can use an upgrade patch to update your operator-based Ansible Automation Platform.

## Patch updating Ansible Automation Platform on OpenShift Container Platform

When you perform a patch update for an installation of Ansible Automation Platform on OpenShift Container Platform, most updates happen within a channel:

1. A new update becomes available in the marketplace (through the redhat-operator CatalogSource).
2. A new InstallPlan is automatically created for your Ansible Automation Platform subscription. If the subscription is set to Manual, the InstallPlan will need to be manually approved in the OpenShift UI. If the subscription is set to Automatic, it will upgrade as soon as the new version is available.

[NOTE]
----
It is recommended that you set a manual install strategy on your Ansible Automation Platform Operator subscription (set when installing or upgrading the Operator) and you will be prompted to approve an upgrade when it becomes available in your selected update channel. Stable channels for each X.Y release (for example, stable-2.5) are available.
----
3. A new Subscription, CSV, and Operator containers will be created alongside the old Subscription, CSV, and containers. Then the old resources will be cleaned up if the new install was successful.

# Installing Red Hat Ansible Automation Platform Operator from the OpenShift Container Platform CLI

Use these instructions to install the Ansible Automation Platform Operator on Red Hat OpenShift Container Platform from the OpenShift Container Platform command-line interface (CLI) using the oc command.

## Prerequisites

* Access to Red Hat OpenShift Container Platform using an account with operator installation permissions.
* The OpenShift Container Platform CLI oc command is installed on your local system. Refer to Installing the OpenShift CLI in the Red Hat OpenShift Container Platform product documentation for further information.

## Subscribing a namespace to an operator using the OpenShift Container Platform CLI

Use this procedure to subscribe a namespace to an operator.


[IMPORTANT]
----
You can only subscribe a single instance of the Ansible Automation Platform Operator into a single namespace.
Subscribing multiple instances in the same namespace can lead to improper operation for both operator instances.
----

1. Create a project for the operator.

```
oc new-project ansible-automation-platform
```

2. Create a file called sub.yaml.
3. Add the following YAML code to the sub.yaml file.

```
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    openshift.io/cluster-monitoring: "true"
  name: ansible-automation-platform
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ansible-automation-platform-operator
  namespace: ansible-automation-platform
spec:
  targetNamespaces:
    - ansible-automation-platform
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ansible-automation-platform
  namespace: ansible-automation-platform
spec:
  channel: 'stable-2.5'
  installPlanApproval: Automatic
  name: ansible-automation-platform-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
---
```


This file creates a Subscription object called ansible-automation-platform that subscribes the ansible-automation-platform namespace to the ansible-automation-platform-operator operator.
4. Run the oc apply command to create the objects specified in the sub.yaml file:

```
oc apply -f sub.yaml
```

5. Verify the CSV PHASE reports "Succeeded" before proceeding using the oc get csv -n ansible-automation-platform command:

```
oc get csv -n ansible-automation-platform

NAME                               DISPLAY                       VERSION              REPLACES                           PHASE
aap-operator.v2.5.0-0.1728520175   Ansible Automation Platform   2.5.0+0.1728520175   aap-operator.v2.5.0-0.1727875185   Succeeded
```

6. Create an AnsibleAutomationPlatform object called example in the ansible-automation-platform namespace.

To change the Ansible Automation Platform and its components from  from example, edit the name field in the metadata: section and replace example with the name you want to use:

```
oc apply -f - <<EOF
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: example
  namespace: ansible-automation-platform
spec:
  # Platform
  image_pull_policy: IfNotPresent
  # Components
  controller:
    disabled: false
  eda:
    disabled: false
  hub:
    disabled: false
    ## Modify to contain your RWM storage class name
    storage_type: file
    file_storage_storage_class: <your-read-write-many-storage-class>
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name
  lightspeed:
    disabled: true
EOF
```


For further information about subscribing namespaces to operators, see Installing from OperatorHub using the CLI in the Red Hat OpenShift Container Platform Operators guide.

You can use the OpenShift Container Platform CLI to fetch the web address and the password of the Automation controller that you created.

## Fetching platform gateway login details from the OpenShift Container Platform CLI

To login to the platform gateway, you need the web address and the password.

### Fetching the platform gateway web address

A Red Hat OpenShift Container Platform route exposes a service at a host name, so that external clients can reach it by name.
When you created the platform gateway instance, a route was created for it.
The route inherits the name that you assigned to the platform gateway object in the YAML file.

Use the following command to fetch the routes:


```
oc get routes -n <platform_namespace>
```


In the following example, the example platform gateway is running in the ansible-automation-platform namespace.


```
$ oc get routes -n ansible-automation-platform

NAME      HOST/PORT                                              PATH   SERVICES          PORT   TERMINATION     WILDCARD
example   example-ansible-automation-platform.apps-crc.testing          example-service   http   edge/Redirect   None
```


The address for the platform gateway instance is example-ansible-automation-platform.apps-crc.testing.

### Fetching the platform gateway password

The YAML block for the platform gateway instance in the AnsibleAutomationPlatform object assigns values to the name and admin_user keys.

1. Use these values in the following command to fetch the password for the platform gateway instance.

```
oc get secret/<your instance name>-<admin_user>-password -o yaml
```

2. The default value for admin_user is admin. Modify the command if you changed the admin username in the AnsibleAutomationPlatform object.

The following example retrieves the password for a platform gateway object called example:

```
oc get secret/example-admin-password -o yaml
```


The base64 encoded password for the platform gateway instance is listed in the metadata field in the output:

```
$ oc get secret/example-admin-password -o yaml

apiVersion: v1
data:
  password: ODzLODzLODzLODzLODzLODzLODzLODzLODzLODzLODzL
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: aap
    app.kubernetes.io/name: example
    app.kubernetes.io/operator-version: ""
    app.kubernetes.io/part-of: example
  name: example-admin-password
  namespace: ansible-automation-platform
```


### Decoding the platform gateway password

After you have found your gateway password, you must decode it from base64.

* Run the following command to decode your password from base64:

```
oc get secret/example-admin-password -o jsonpath={.data.password} | base64 --decode
```


## Additional resources

* For more information on running operators on OpenShift Container Platform, navigate to the OpenShift Container Platform product documentation and click the Operators - Working with Operators in OpenShift Container Platform guide.

# Configuring the Red Hat Ansible Automation Platform Operator on Red Hat OpenShift Container Platform

The  platform gateway for Ansible Automation Platform enables you to manage the following Ansible Automation Platform components to form a single user interface:

* Automation controller
* Automation hub
* Event-Driven Ansible
* Red Hat Ansible Lightspeed (This feature is disabled by default, you must opt in to use it.)

Before you can deploy the platform gateway you must have Ansible Automation Platform Operator installed in a namespace.
If you have not installed Ansible Automation Platform Operator see Installing the Red Hat Ansible Automation Platform Operator on Red Hat OpenShift Container Platform.


[NOTE]
----
Platform gateway is only available under Ansible Automation Platform Operator version 2.5. Every component deployed under Ansible Automation Platform Operator 2.5 defaults to version 2.5.
----

If you have the Ansible Automation Platform Operator and some or all of the Ansible Automation Platform components installed see Deploying the platform gateway with existing Ansible Automation Platform components for how to proceed.

## Linking your components to the platform gateway

After installing the Ansible Automation Platform Operator in your namespace you can set up your Ansible Automation Platform instance.
Then link all the platform components to a single user interface.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Details tab.
5. On the Ansible Automation Platform tile click Create instance.
6. From the Create Ansible Automation Platform page enter a name for your instance in the Name field.
7. Click YAML view and paste the following:

```
spec:
  database:
    resource_requirements:
      requests:
        cpu: 200m
        memory: 512Mi
    storage_requirements:
      requests:
        storage: 100Gi

  controller:
    disabled: false

  eda:
    disabled: false

  hub:
    disabled: false
    storage_type: file
    file_storage_storage_class: <read-write-many-storage-class>
    file_storage_size: 10Gi
```

8. Click Create.

Go to your Ansible Automation Platform Operator deployment and click All instances to verify if all instances deployed correctly.
You should see the Ansible Automation Platform instance and the deployed AutomationController, EDA, and AutomationHub instances here.

Alternatively you can check by the command line, run: oc get route

## Accessing the platform gateway

You should use the Ansible Automation Platform instance as your default.
This instance links the automation controller, automation hub, and Event-Driven Ansible deployments to a single interface.

To access your Ansible Automation Platform instance:

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to
3. Click the link under Location for Ansible Automation Platform.
4. This redirects you to the Ansible Automation Platform login page. Enter "admin" as your username in the Username field.
5. For the password you need to:
1. Go to to .
2. Click <your instance name>-admin-password and copy the password.
3. Paste the password into the Password field.
6. Click Login.
7. Apply your subscription:
1. Click Subscription manifest or Username/password.
2. Upload your manifest or enter your username and password.
3. Select  your subscription from the Subscription list.
4. Click Next.
This redirects you to the Analytics page.
8. Click Next.
9. Select the I agree to the terms of the license agreement checkbox.
10. Click Next.

You now have access to the platform gateway user interface.
If you cannot access the Ansible Automation Platform see Frequently asked questions on platform gateway for help with troubleshooting and debugging.

## Deploying the platform gateway with existing Ansible Automation Platform components

You can link any components of the Ansible Automation Platform, that you have already installed to a new Ansible Automation Platform instance.

The following procedure simulates a scenario where you have automation controller as an existing component and want to add automation hub and Event-Driven Ansible.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Click Subscriptions and edit your Update channel to stable-2.5.
5. Click Details and on the Ansible Automation Platform tile click Create instance.
6. From the Create Ansible Automation Platform page enter a name for your instance in the Name field.
7. Click YAML view and copy in the following:

```
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: example-aap
  namespace: aap
spec:
  database:
    resource_requirements:
      requests:
        cpu: 200m
        memory: 512Mi
    storage_requirements:
      requests:
        storage: 100Gi

  # Platform
  image_pull_policy: IfNotPresent

  # Components
  controller:
    disabled: false
    name: existing-controller-name
  eda:
    disabled: false
  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: <your-read-write-many-storage-class>
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage
```

1. For new components, if you do not specify a name, a default name is generated.
8. Click Create.
9. To access your new instance, see Accessing the platform gateway.


[NOTE]
----
If you have an existing controller with a managed Postgres pod, after creating the Ansible Automation Platform resource your automation controller instance will continue to use that original Postgres pod. If you were to do a fresh install you would have a single Postgres managed pod for all instances.
----

## Configuring an external database for platform gateway on Red Hat Ansible Automation Platform Operator

For users who prefer to deploy Ansible Automation Platform with an external database, they can do so by configuring a secret with instance credentials and connection information, then applying it to their cluster using the oc create command.

By default, the Ansible Automation Platform Operator automatically creates and configures a managed PostgreSQL pod in the same namespace as your Ansible Automation Platform deployment. You can deploy Ansible Automation Platform with an external database instead of the managed PostgreSQL pod that the Ansible Automation Platform Operator automatically creates.

Using an external database lets you share and reuse resources and manually manage backups, upgrades, and performance optimizations.


[NOTE]
----
The same external database (PostgreSQL instance) can be used for both automation hub, automation controller, and platform gateway as long as the database names are different. In other words, you can have multiple databases with different names inside a single PostgreSQL instance.
----

The following section outlines the steps to configure an external database for your platform gateway on a Ansible Automation Platform Operator.

The external database must be a PostgreSQL database that is the version supported by the current release of Ansible Automation Platform.


[NOTE]
----
Ansible Automation Platform 2.5 supports PostgreSQL 15.
----

The external postgres instance credentials and connection information must be stored in a secret, which is then set on the platform gateway spec.

1. Create a postgres_configuration_secret YAML file, following the template below:

```
apiVersion: v1
kind: Secret
metadata:
  name: external-postgres-configuration
  namespace: <target_namespace> 1
stringData:
  host: "<external_ip_or_url_resolvable_by_the_cluster>" 2
  port: "<external_port>" 3
  database: "<desired_database_name>"
  username: "<username_to_connect_as>"
  password: "<password_to_connect_with>" 4
  sslmode: "prefer" 5
  type: "unmanaged"
type: Opaque
```

Namespace to create the secret in. This should be the same namespace you want to deploy to.
The resolvable hostname for your database node.
External port defaults to 5432.
Value for variable password should not contain single or double quotes (', ") or backslashes (\) to avoid any issues during deployment, backup or restoration.
The variable sslmode is valid for external databases only. The allowed values are: prefer, disable, allow, require, verify-ca, and verify-full.
2. Apply external-postgres-configuration-secret.yml to your cluster using the oc create command.

```
$ oc create -f external-postgres-configuration-secret.yml
```

3. When creating your AnsibleAutomationPlatform custom resource object, specify the secret on your spec, following the example below:

```
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: example-aap
  Namespace: aap
spec:
  database:
     database_secret: automation-platform-postgres-configuration
```


## Enabling HTTPS redirect for single sign-on (SSO) for platform gateway on OpenShift Container Platform

HTTPS redirect for SAML, allows you to log in once and access all of the platform gateway without needing to reauthenticate.

* You have successfully configured SAML in the gateway from the Ansible Automation Platform Operator. Refer to Configuring SAML authentication for help with this.

1. Log in to Red Hat OpenShift Container Platform.
2. Go to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select All Instances and go to your AnsibleAutomationPlatform instance.
5. Click the  &#8942; icon and then select Edit AnsibleAutomationPlatform.
6. In the YAML view paste the following YAML code under the spec: section:

```
spec:
  extra_settings:
    - setting: REDIRECT_IS_HTTPS
      value: '"True"'
```

7. Click Save.

After you have added the REDIRECT_IS_HTTPS setting, wait for the pod to redeploy automatically. You can verify this setting makes it into the pod by running:


```
oc exec -it <gateway-pod-name> -- grep REDIRECT /etc/ansible-automation-platform/gateway/settings.py
```


## Frequently asked questions on platform gateway

If I delete my Ansible Automation Platform deployment will I still have access to Automation Controller?:: No, automation controller, automation hub, and Event-Driven Ansible are nested within the deployment and are also deleted.
Something went wrong with my deployment but I'm not sure what, how can I find out?:: You can follow along in the command line while the operator is reconciling, this can be helpful for debugging.
Alternatively you can click into the deployment instance to see the status conditions being updated as the deployment goes on.
Is it still possible to view individual component logs?:: When troubleshooting you should examine the Ansible Automation Platform instance for the main logs and then each individual component (EDA, AutomationHub, AutomationController) for more specific information.
Where can I view the condition of an instance?:: To display status conditions click into the instance, and look under the Details or Events tab.
Alternatively, to display the status conditions you can run the get command:
oc get automationcontroller <instance-name> -o jsonpath=Pipe "| jq"
Can I track my migration in real time?:: To help track the status of the migration or to understand why migration might have failed you can look at the migration logs as they are running. Use the logs command:
oc logs fresh-install-controller-migration-4.6.0-jwfm6 -f
I have configured my SAML but authentication fails with this error: "Unable to complete social auth login" What can I do?:: You must update your Ansible Automation Platform instance to include the REDIRECT_IS_HTTPS extra setting. See Enabling single sign-on (SSO) for platform gateway on OpenShift Container Platform for help with this.

# Configuring automation controller on Red Hat OpenShift Container Platform web console

You can use these instructions to configure the automation controller operator on Red Hat OpenShift Container Platform, specify custom resources, and deploy Ansible Automation Platform with an external database.

Automation controller configuration can be done through the automation controller extra_settings or directly in the user interface after deployment. However, it is important to note that configurations made in extra_settings take precedence over settings made in the user interface.


[NOTE]
----
When an instance of automation controller is removed, the associated PVCs are not automatically deleted. This can cause issues during migration if the new deployment has the same name as the previous one. Therefore, it is recommended that you manually remove old PVCs before deploying a new automation controller instance in the same namespace. See Finding and deleting PVCs for more information.
----

## Prerequisites

* You have installed the Red Hat Ansible Automation Platform catalog in Operator Hub.
* For automation controller, a default StorageClass must be configured on the cluster for the operator to dynamically create needed PVCs. This is not necessary if an external PostgreSQL database is configured.
* For Hub a StorageClass that supports ReadWriteMany must be available on the cluster to dynamically created the PVC needed for the content, redis and api pods. If it is not the default StorageClass on the cluster, you can specify it when creating your AutomationHub object.

### Configuring your controller image pull policy

Use this procedure to configure the image pull policy on your automation controller.

1. Log in to Red Hat OpenShift Container Platform.
2. Go to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation Controller tab.
5. For new instances, click Create AutomationController.
1. For existing instances, you can edit the YAML view by clicking the  &#8942; icon and then Edit AutomationController.
6. Click advanced Configuration.
Under Image Pull Policy, click on the radio button to select
* Always
* Never
* IfNotPresent
7. To display the option under Image Pull Secrets, click the arrow.
1. Click + beside Add Image Pull Secret and enter a value.
8. To display fields under the Web container resource requirements drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
9. To display fields under the Task container resource requirements drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
10. To display fields under the EE Control Plane container resource requirements drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
11. To display fields under the PostgreSQL init container resource requirements (when using a managed service) drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
12. To display fields under the Redis container resource requirements drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
13. To display fields under the PostgreSQL container resource requirements (when using a managed instance)* drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
14. To display the PostgreSQL container storage requirements (when using a managed instance) drop-down list, click the arrow.
1. Under Limits, and Requests, enter values for CPU cores, Memory, and Storage.
15. Under Replicas, enter the number of instance replicas.
16. Under Remove used secrets on instance removal, select true or false. The default is false.
17. Under Preload instance with data upon creation, select true or false. The default is true.

### Configuring your controller LDAP security

You can configure your LDAP SSL configuration for automation controller through any of the following options:

* The automation controller user interface.
* The platform gateway user interface. See the Configuring LDAP authentication section of the Access management and authentication guide for additional steps.
* The following procedure steps.

1. If you do not have a ldap_cacert_secret, you can create one with the following command:

```
$ oc create secret generic <resourcename>-custom-certs \
    --from-file=ldap-ca.crt=<PATH/TO/YOUR/CA/PEM/FILE>  \ 1
```

Modify this to point to where your CA cert is stored.

This will create a secret that looks like this:

```
$ oc get secret/mycerts -o yaml
apiVersion: v1
data:
  ldap-ca.crt: <mysecret> 1
kind: Secret
metadata:
  name: mycerts
  namespace: awx
type: Opaque
```

Automation controller looks for the data field ldap-ca.crt in the specified secret when using the ldap_cacert_secret.
2. Under LDAP Certificate Authority Trust Bundle click the drop-down menu and select your ldap_cacert_secret.
3. Under LDAP Password Secret, click the drop-down menu and select a secret.
4. Under EE Images Pull Credentials Secret, click the drop-down menu and select a secret.
5. Under Bundle Cacert Secret, click the drop-down menu and select a secret.
6. Under Service Type, click the drop-down menu and select
* ClusterIP
* LoadBalancer
* NodePort

### Configuring your automation controller operator route options

The Red Hat Ansible Automation Platform operator installation form allows you to further configure your automation controller operator route options under Advanced configuration.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation Controller tab.
5. For new instances, click Create AutomationController.
1. For existing instances, you can edit the YAML view by clicking the &#8942; icon and then Edit AutomationController.
6. Click Advanced configuration.
7. Under Ingress type, click the drop-down menu and select Route.
8. Under Route DNS host, enter a common host name that the route answers to.
9. Under Route TLS termination mechanism, click the drop-down menu and select Edge or Passthrough. For most instances Edge should be selected.
10. Under Route TLS credential secret, click the drop-down menu and select a secret from the list.
11. Under Enable persistence for /var/lib/projects directory select either true or false by moving the slider.

### Configuring the ingress type for your automation controller operator

The Ansible Automation Platform Operator installation form allows you to further configure your automation controller operator ingress under Advanced configuration.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation Controller tab.
5. For new instances, click Create AutomationController.
1. For existing instances, you can edit the YAML view by clicking the &#8942; icon and then Edit AutomationController.
6. Click Advanced configuration.
7. Under Ingress type, click the drop-down menu and select Ingress.
8. Under Ingress annotations, enter any annotations to add to the ingress.
9. Under Ingress TLS secret, click the drop-down menu and select a secret from the list.

After you have configured your automation controller operator, click Create at the bottom of the form view. Red Hat OpenShift Container Platform will now create the pods. This may take a few minutes.

You can view the progress by navigating to  and locating the newly created instance.

Verify that the following operator pods provided by the Ansible Automation Platform Operator installation from automation controller are running:




[NOTE]
----
A missing pod can indicate the need for a pull secret. Pull secrets are required for protected or private image registries. See Using image pull secrets for more information. You can diagnose this issue further by running oc describe pod <pod-name> to see if there is an ImagePullBackOff error on that pod.
----

## Configuring an external database for automation controller on Red Hat Ansible Automation Platform Operator

For users who prefer to deploy Ansible Automation Platform with an external database, they can do so by configuring a secret with instance credentials and connection information, then applying it to their cluster using the oc create command.

By default, the Ansible Automation Platform Operator automatically creates and configures a managed PostgreSQL pod in the same namespace as your Ansible Automation Platform deployment. You can deploy Ansible Automation Platform with an external database instead of the managed PostgreSQL pod that the Ansible Automation Platform Operator automatically creates.

Using an external database lets you share and reuse resources and manually manage backups, upgrades, and performance optimizations.


[NOTE]
----
The same external database (PostgreSQL instance) can be used for both automation hub, automation controller, and platform gateway as long as the database names are different. In other words, you can have multiple databases with different names inside a single PostgreSQL instance.
----

The following section outlines the steps to configure an external database for your automation controller on a Ansible Automation Platform Operator.

The external database must be a PostgreSQL database that is the version supported by the current release of Ansible Automation Platform.


[NOTE]
----
Ansible Automation Platform 2.5 supports PostgreSQL 15.
----

The external postgres instance credentials and connection information must be stored in a secret, which is then set on the automation controller spec.

1. Create a postgres_configuration_secret YAML file, following the template below:

```
apiVersion: v1
kind: Secret
metadata:
  name: external-postgres-configuration
  namespace: <target_namespace> 1
stringData:
  host: "<external_ip_or_url_resolvable_by_the_cluster>" 2
  port: "<external_port>" 3
  database: "<desired_database_name>"
  username: "<username_to_connect_as>"
  password: "<password_to_connect_with>" 4
  sslmode: "prefer" 5
  type: "unmanaged"
type: Opaque
```

Namespace to create the secret in. This should be the same namespace you want to deploy to.
The resolvable hostname for your database node.
External port defaults to 5432.
Value for variable password should not contain single or double quotes (', ") or backslashes (\) to avoid any issues during deployment, backup or restoration.
The variable sslmode is valid for external databases only. The allowed values are: prefer, disable, allow, require, verify-ca, and verify-full.
2. Apply external-postgres-configuration-secret.yml to your cluster using the oc create command.

```
$ oc create -f external-postgres-configuration-secret.yml
```

3. When creating your AutomationController custom resource object, specify the secret on your spec, following the example below:

```
apiVersion: automationcontroller.ansible.com/v1beta1
kind: AutomationController
metadata:
  name: controller-dev
spec:
  postgres_configuration_secret: external-postgres-configuration
```


## Finding and deleting PVCs

A persistent volume claim (PVC) is a storage volume used to store data that automation hub and automation controller applications use. These PVCs are independent from the applications and remain even when the application is deleted. If you are confident that you no longer need a PVC, or have backed it up elsewhere, you can manually delete them.

1. List the existing PVCs in your deployment namespace:

```
oc get pvc -n <namespace>
```

2. Identify the PVC associated with your previous deployment by comparing the old deployment name and the PVC name.
3. Delete the old PVC:

```
oc delete pvc -n <namespace> <pvc-name>
```


## Additional resources

* For more information on running operators on OpenShift Container Platform, navigate to the OpenShift Container Platform product documentation and click the Operators - Working with Operators in OpenShift Container Platform guide.

# Configuring automation hub on Red Hat OpenShift Container Platform web console

You can use these instructions to configure the automation hub operator on Red Hat OpenShift Container Platform, specify custom resources, and deploy Ansible Automation Platform with an external database.

Automation hub configuration can be done through the automation hub pulp_settings or directly in the user interface after deployment. However, it is important to note that configurations made in pulp_settings take precedence over settings made in the user interface. Hub settings should always be set as lowercase on the Hub custom resource specification.


[NOTE]
----
When an instance of automation hub is removed, the PVCs are not automatically deleted. This can cause issues during migration if the new deployment has the same name as the previous one. Therefore, it is recommended that you manually remove old PVCs before deploying a new automation hub instance in the same namespace. See Finding and deleting PVCs for more information.
----

## Prerequisites

* You have installed the Ansible Automation Platform Operator in Operator Hub.

### Storage options for Ansible Automation Platform Operator installation on Red Hat OpenShift Container Platform

Automation hub requires ReadWriteMany file-based storage, Azure Blob storage, or Amazon S3-compliant storage for operation so that multiple pods can access shared content, such as collections.

The process for configuring object storage on the AutomationHub CR is similar for Amazon S3 and Azure Blob Storage.

If you are using file-based storage and your installation scenario includes automation hub, ensure that the storage option for Ansible Automation Platform Operator is set to ReadWriteMany.
ReadWriteMany is the default storage option.

In addition, OpenShift Data Foundation provides a ReadWriteMany or S3-compliant implementation. Also, you can set up NFS storage configuration to support ReadWriteMany. This, however, introduces the NFS server as a potential, single point of failure.

* Persistent storage using NFS in the OpenShift Container Platform Storage guide
* IBM's How do I create a storage class for NFS dynamic storage provisioning in an OpenShift environment?

#### Provisioning OCP storage with ReadWriteMany access mode

To ensure successful installation of Ansible Automation Platform Operator, you must provision your storage type for automation hub initially to ReadWriteMany access mode.

1. Go to .
2. Click Create PersistentVolume.
3. In the first step, update the accessModes from the default ReadWriteOnce to ReadWriteMany.
1. See Provisioning to update the access mode. for a detailed overview.
4. Complete the additional steps in this section to create the persistent volume claim (PVC).

#### Configuring object storage on Amazon S3

Red Hat supports Amazon Simple Storage Service (S3) for automation hub.
You can configure it when deploying the AutomationHub custom resource (CR), or you can configure it for an existing instance.

* Create an Amazon S3 bucket to store the objects.
* Note the name of the S3 bucket.

1. Create a Kubernetes secret containing the AWS credentials and connection details, and the name of your Amazon S3 bucket.
The following example creates a secret called test-s3:

```yaml
$ oc -n $HUB_NAMESPACE apply -f- <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: 'test-s3'
stringData:
  s3-access-key-id: $S3_ACCESS_KEY_ID
  s3-secret-access-key: $S3_SECRET_ACCESS_KEY
  s3-bucket-name: $S3_BUCKET_NAME
  s3-region: $S3_REGION
EOF
```

2. Add the secret to the automation hub custom resource (CR) spec:

```yaml
spec:
  object_storage_s3_secret: test-s3
```

3. If you are applying this secret to an existing instance, restart the API pods for the change to take effect.
<hub-name> is the name of your hub instance.


```bash
$ oc -n $HUB_NAMESPACE delete pod -l app.kubernetes.io/name=<hub-name>-api
```


#### Configuring object storage on Azure Blob

Red Hat supports Azure Blob Storage for automation hub.
You can configure it when deploying the AutomationHub custom resource (CR), or you can configure it for an existing instance.

* Create an Azure Storage blob container to store the objects.
* Note the name of the blob container.

1. Create a Kubernetes secret containing the credentials and connection details for your Azure account, and the name of your Azure Storage blob container.
The following example creates a secret called test-azure:

```yaml
$ oc -n $HUB_NAMESPACE apply -f- <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: 'test-azure'
stringData:
  azure-account-name: $AZURE_ACCOUNT_NAME
  azure-account-key: $AZURE_ACCOUNT_KEY
  azure-container: $AZURE_CONTAINER
  azure-container-path: $AZURE_CONTAINER_PATH
  azure-connection-string: $AZURE_CONNECTION_STRING
EOF
```

2. Add the secret to the automation hub custom resource (CR) spec:

```yaml
spec:
  object_storage_azure_secret: test-azure
```

3. If you are applying this secret to an existing instance, restart the API pods for the change to take effect.
<hub-name> is the name of your hub instance.


```bash
$ oc -n $HUB_NAMESPACE delete pod -l app.kubernetes.io/name=<hub-name>-api
```


### Configure your automation hub operator route options

The Red Hat Ansible Automation Platform operator installation form allows you to further configure your automation hub operator route options under Advanced configuration.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation Hub tab.
5. For new instances, click Create AutomationHub.
1. For existing instances, you can edit the YAML view by clicking the &#8942; icon and then Edit AutomationHub.
6. Click Advanced configuration.
7. Under Ingress type, click the drop-down menu and select Route.
8. Under Route DNS host, enter a common host name that the route answers to.
9. Under Route TLS termination mechanism, click the drop-down menu and select Edge or Passthrough.
10. Under Route TLS credential secret, click the drop-down menu and select a secret from the list.

### Configuring the ingress type for your automation hub operator

The Ansible Automation Platform Operator installation form allows you to further configure your automation hub operator ingress under Advanced configuration.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation Hub tab.
5. For new instances, click Create AutomationHub.
1. For existing instances, you can edit the YAML view by clicking the &#8942; icon and then Edit AutomationHub.
6. Click Advanced Configuration.
7. Under Ingress type, click the drop-down menu and select Ingress.
8. Under Ingress annotations, enter any annotations to add to the ingress.
9. Under Ingress TLS secret, click the drop-down menu and select a secret from the list.

After you have configured your automation hub operator, click Create at the bottom of the form view. Red Hat OpenShift Container Platform will now create the pods. This may take a few minutes.

You can view the progress by navigating to  and locating the newly created instance.

Verify that the following operator pods provided by the Ansible Automation Platform Operator installation from automation hub are running:




[NOTE]
----
A missing pod can indicate the need for a pull secret. Pull secrets are required for protected or private image registries. See Using image pull secrets for more information. You can diagnose this issue further by running oc describe pod <pod-name> to see if there is an ImagePullBackOff error on that pod.
----

## Configuring LDAP authentication for Ansible automation hub on OpenShift Container Platform

Configure LDAP authentication settings for Ansible Automation Platform on OpenShift Container Platform in the spec section of your Hub instance configuration file.

* Use the following example to configure LDAP in your automation hub instance. For any blank fields, enter ``.

```
spec:
    pulp_settings:
      auth_ldap_user_attr_map:
        email: "mail"
        first_name: "givenName"
        last_name: "sn"
      auth_ldap_group_search_base_dn: 'cn=groups,cn=accounts,dc=example,dc=com'
      auth_ldap_bind_dn: ' '
      auth_ldap_bind_password: ' '
      auth_ldap_group_search_filter: (objectClass=posixGroup)
      auth_ldap_user_search_scope: SUBTREE
      auth_ldap_server_uri: 'ldap://ldapserver:389'
      authentication_backend_preset: ldap
      auth_ldap_mirror_groups: 'True'
      auth_ldap_user_search_base_dn: 'cn=users,cn=accounts,dc=example,dc=com'
      auth_ldap_bind_password: 'ldappassword'
      auth_ldap_user_search_filter: (uid=%(user)s)
      auth_ldap_group_search_scope: SUBTREE
      auth_ldap_user_flags_by_group: '@json {"is_superuser": "cn=tower-admin,cn=groups,cn=accounts,dc=example,dc=com"}'
```



[NOTE]
----
Do not leave any fields empty. For fields with no variable, enter `` to indicate a default value.
----

## Finding the automation hub route

You can access the automation hub through the platform gateway or through the following procedure.

1. Log into Red Hat OpenShift Container Platform.
2. Navigate to .
3. Under Location, click on the URL for your automation hub instance.

The automation hub user interface launches where you can sign in with the administrator credentials specified during the operator configuration process.


[NOTE]
----
If you did not specify an administrator password during configuration, one was automatically created for you. To locate this password, go to your project, select  and open controller-admin-password. From there you can copy the password and paste it into the Automation hub password field.
----

## Configuring an external database for automation hub on Red Hat Ansible Automation Platform Operator

For users who prefer to deploy Ansible Automation Platform with an external database, they can do so by configuring a secret with instance credentials and connection information, then applying it to their cluster using the oc create command.

By default, the Ansible Automation Platform Operator automatically creates and configures a managed PostgreSQL pod in the same namespace as your Ansible Automation Platform deployment.

You can choose to use an external database instead if you prefer to use a dedicated node to ensure dedicated resources or to manually manage backups, upgrades, or performance tweaks.


[NOTE]
----
The same external database (PostgreSQL instance) can be used for both automation hub, automation controller, and platform gateway as long as the database names are different. In other words, you can have multiple databases with different names inside a single PostgreSQL instance.
----

The following section outlines the steps to configure an external database for your automation hub on a Ansible Automation Platform Operator.

The external database must be a PostgreSQL database that is the version supported by the current release of Ansible Automation Platform.


[NOTE]
----
Ansible Automation Platform 2.5 supports PostgreSQL 15.
----

The external postgres instance credentials and connection information will need to be stored in a secret, which will then be set on the automation hub spec.

1. Create a postgres_configuration_secret YAML file, following the template below:

```
apiVersion: v1
kind: Secret
metadata:
  name: external-postgres-configuration
  namespace: <target_namespace> 1
stringData:
  host: "<external_ip_or_url_resolvable_by_the_cluster>" 2
  port: "<external_port>" 3
  database: "<desired_database_name>"
  username: "<username_to_connect_as>"
  password: "<password_to_connect_with>" 4
  sslmode: "prefer" 5
  type: "unmanaged"
type: Opaque
```

Namespace to create the secret in. This should be the same namespace you want to deploy to.
The resolvable hostname for your database node.
External port defaults to 5432.
Value for variable password should not contain single or double quotes (', ") or backslashes (\) to avoid any issues during deployment, backup or restoration.
The variable sslmode is valid for external databases only. The allowed values are: prefer, disable, allow, require, verify-ca, and verify-full.
2. Apply external-postgres-configuration-secret.yml to your cluster using the oc create command.

```
$ oc create -f external-postgres-configuration-secret.yml
```

3. When creating your AutomationHub custom resource object, specify the secret on your spec, following the example below:

```
apiVersion: automationhub.ansible.com/v1beta1
kind: AutomationHub
metadata:
  name: hub-dev
spec:
  postgres_configuration_secret: external-postgres-configuration
```


### Enabling the hstore extension for the automation hub PostgreSQL database

Added in Ansible Automation Platform 2.5, the database migration script uses hstore fields to store information, therefore the hstore extension to the automation hub PostgreSQL database must be enabled.

This process is automatic when using the Ansible Automation Platform installer and a managed PostgreSQL server.

If the PostgreSQL database is external, you must enable the hstore extension to the automation hub PostreSQL database manually before automation hub installation.

If the hstore extension is not enabled before automation hub installation, a failure is raised during database migration.

1. Check if the extension is available on the PostgreSQL server (automation hub database).

```
$ psql -d <automation hub database> -c "SELECT * FROM pg_available_extensions WHERE name='hstore'"
```

2. Where the default value for <automation hub database> is automationhub.

Example output with hstore available:

```
name  | default_version | installed_version |comment
------+-----------------+-------------------+---------------------------------------------------
 hstore | 1.7           |                   | data type for storing sets of (key, value) pairs
(1 row)
```


Example output with hstore not available:

```
 name | default_version | installed_version | comment
------+-----------------+-------------------+---------
(0 rows)
```

3. On a RHEL based server, the hstore extension is included in the postgresql-contrib RPM package, which is not installed automatically when installing the PostgreSQL server RPM package.

To install the RPM package, use the following command:

```
dnf install postgresql-contrib
```

4. Create the hstore PostgreSQL extension on the automation hub database with the following command:

```
$ psql -d <automation hub database> -c "CREATE EXTENSION hstore;"
```


The output of which is:

```
CREATE EXTENSION
```


In the following output, the installed_version field contains the hstore extension used, indicating that hstore is enabled.

```
name | default_version | installed_version | comment
-----+-----------------+-------------------+------------------------------------------------------
hstore  |     1.7      |       1.7         | data type for storing sets of (key, value) pairs
(1 row)
```


## Finding and deleting PVCs

A persistent volume claim (PVC) is a storage volume used to store data that automation hub and automation controller applications use. These PVCs are independent from the applications and remain even when the application is deleted. If you are confident that you no longer need a PVC, or have backed it up elsewhere, you can manually delete them.

1. List the existing PVCs in your deployment namespace:

```
oc get pvc -n <namespace>
```

2. Identify the PVC associated with your previous deployment by comparing the old deployment name and the PVC name.
3. Delete the old PVC:

```
oc delete pvc -n <namespace> <pvc-name>
```


## Additional configurations

A collection download count can help you understand collection usage. To add a collection download count to automation hub, set the following configuration:


```
spec:
  pulp_settings:
    ansible_collect_download_count: true
```


When ansible_collect_download_count is enabled, automation hub will display a download count by the collection.

## Additional resources

* For more information on running operators on OpenShift Container Platform, navigate to the OpenShift Container Platform product documentation and click the Operators - Working with Operators in OpenShift Container Platform guide.

# Deploying clustered Redis on Red Hat Ansible Automation Platform Operator

When you create an Ansible Automation Platform instance through the Ansible Automation Platform Operator, standalone Redis is assigned by default.
To deploy clustered Redis, use the following procedure.

For more information about Redis, refer to Caching and queueing system in the Planning your installation guide.

* You have installed an Ansible Automation Platform Operator deployment.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Details tab.
5. On the Ansible Automation Platform tile click Create instance.
1. For existing instances, you can edit the YAML view by clicking the &#8942; icon and then Edit AnsibleAutomationPlatform.
2. Change the redis_mode value to "clustered".
3. Click Reload, then Save.
6. Click to expand Advanced configuration.
7. For the Redis Mode list, select Cluster.
8. Configure the rest of your instance as necessary, then click Create.

Your instance will deploy with a cluster Redis with 6 Redis replicas as default.

# Using Red Hat Single Sign-On Operator with automation hub

Private automation hub uses Red Hat Single Sign-On for authentication.

The Red Hat Single Sign-On Operator creates and manages resources.
Use this operator to create custom resources to automate Red Hat Single Sign-On administration in OpenShift.

* When installing Ansible Automation Platform on Virtual Machines (VMs) the installer can automatically install and configure Red Hat Single Sign-On for use with private automation hub.
* When installing Ansible Automation Platform on Red Hat OpenShift Container Platform you must install Single Sign-On separately.

This chapter describes the process to configure Red Hat Single Sign-On and integrate it with private automation hub when Ansible Automation Platform is installed on OpenShift Container Platform.

* You have access to Red Hat OpenShift Container Platform using an account with operator installation permissions.
* You have installed the catalog containing the Red Hat Ansible Automation Platform operators.
* You have installed the Red Hat Single Sign-On Operator.
To install the Red Hat Single Sign-On Operator, follow the procedure in Installing Red Hat Single Sign-On using a custom resource in the Red Hat Single Sign-On documentation.

## Creating a Keycloak instance

After you install the Red Hat Single Sign-On Operator, you can create a Keycloak instance for use with Ansible Automation Platform.

From here you provide an external Postgres or one will be created for you.

/ Log in to Red Hat OpenShift Container Platform.
. Navigate to .
. Select the RH-SSO project.
. Select the Red Hat Single Sign-On Operator.
. On the Red Hat Single Sign-On Operator details page select Keycloak.
. Click Create instance.
. Click YAML view.

+
The default Keycloak custom resource is as follows:

+


```
apiVersion: keycloak.org/v1alpha1
kind: Keycloak
metadata:
  name: example-keycloak
  labels:
	app: sso
  namespace: aap
spec:
  externalAccess:
	enabled: true
  instances: 1
```


+
. Click Create.

1. When deployment is complete, you can use this credential to login to the administrative console.
2. You can find the credentials for the administrator in the credential-<custom-resource> (example keycloak) secret in the namespace.

## Creating a Keycloak realm for Ansible Automation Platform

Create a realm to manage a set of users, credentials, roles, and groups.
A user belongs to and logs into a realm.
Realms are isolated from one another and can only manage and authenticate the users that they control.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Red Hat Single Sign-On Operator project.
4. Select the Keycloak Realm tab and click Create Keycloak Realm.
5. On the Keycloak Realm form, select YAML view.
Edit the YAML file as follows:

```
kind: KeycloakRealm
apiVersion: keycloak.org/v1alpha1
metadata:
  name: ansible-automation-platform-keycloakrealm
  namespace: rh-sso
  labels:
    app: sso
    realm: ansible-automation-platform
spec:
  realm:
    id: ansible-automation-platform
    realm: ansible-automation-platform
    enabled: true
    displayName: Ansible Automation Platform
  instanceSelector:
    matchLabels:
      app: sso
```


6. Click Create and wait for the process to complete.

## Creating a Keycloak client

Keycloak clients authenticate hub users with Red Hat Single Sign-On.
When a user authenticates the request goes through the Keycloak client.
When Single Sign-On validates or issues the OAuth token, the client provides the response to automation hub and the user can log in.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Red Hat Single Sign-On Operator project.
4. Select the Keycloak Client tab and click Create Keycloak Client.
5. On the Keycloak Realm form, select YAML view.
6. Replace the default YAML file with the following:

```
kind: KeycloakClient
apiVersion: keycloak.org/v1alpha1
metadata:
  name: automation-hub-client-secret
  labels:
    app: sso
    realm: ansible-automation-platform
  namespace: rh-sso
spec:
  realmSelector:
    matchLabels:
      app: sso
      realm: ansible-automation-platform
  client:
    name: Automation Hub
    clientId: automation-hub
    secret: <client-secret>                        1
    clientAuthenticatorType: client-secret
    description: Client for automation hub
    attributes:
      user.info.response.signature.alg: RS256
      request.object.signature.alg: RS256
    directAccessGrantsEnabled: true
    publicClient: true
    protocol: openid-connect
    standardFlowEnabled: true
    protocolMappers:
      - config:
          access.token.claim: "true"
          claim.name: "family_name"
          id.token.claim: "true"
          jsonType.label: String
          user.attribute: lastName
          userinfo.token.claim: "true"
        consentRequired: false
        name: family name
        protocol: openid-connect
        protocolMapper: oidc-usermodel-property-mapper
      - config:
          userinfo.token.claim: "true"
          user.attribute: email
          id.token.claim: "true"
          access.token.claim: "true"
          claim.name: email
          jsonType.label: String
        name: email
        protocol: openid-connect
        protocolMapper: oidc-usermodel-property-mapper
        consentRequired: false
      - config:
          multivalued: "true"
          access.token.claim: "true"
          claim.name: "resource_access.${client_id}.roles"
          jsonType.label: String
        name: client roles
        protocol: openid-connect
        protocolMapper: oidc-usermodel-client-role-mapper
        consentRequired: false
      - config:
          userinfo.token.claim: "true"
          user.attribute: firstName
          id.token.claim: "true"
          access.token.claim: "true"
          claim.name: given_name
          jsonType.label: String
        name: given name
        protocol: openid-connect
        protocolMapper: oidc-usermodel-property-mapper
        consentRequired: false
      - config:
          id.token.claim: "true"
          access.token.claim: "true"
          userinfo.token.claim: "true"
        name: full name
        protocol: openid-connect
        protocolMapper: oidc-full-name-mapper
        consentRequired: false
      - config:
          userinfo.token.claim: "true"
          user.attribute: username
          id.token.claim: "true"
          access.token.claim: "true"
          claim.name: preferred_username
          jsonType.label: String
        name: <username>
        protocol: openid-connect
        protocolMapper: oidc-usermodel-property-mapper
        consentRequired: false
      - config:
          access.token.claim: "true"
          claim.name: "group"
          full.path: "true"
          id.token.claim: "true"
          userinfo.token.claim: "true"
        consentRequired: false
        name: group
        protocol: openid-connect
        protocolMapper: oidc-group-membership-mapper
      - config:
          multivalued: 'true'
          id.token.claim: 'true'
          access.token.claim: 'true'
          userinfo.token.claim: 'true'
          usermodel.clientRoleMapping.clientId:  'automation-hub'
          claim.name: client_roles
          jsonType.label: String
        name: client_roles
        protocolMapper: oidc-usermodel-client-role-mapper
        protocol: openid-connect
      - config:
          id.token.claim: "true"
          access.token.claim: "true"
          included.client.audience: 'automation-hub'
        protocol: openid-connect
        name: audience mapper
        protocolMapper: oidc-audience-mapper
  roles:
    - name: "hubadmin"
      description: "An administrator role for automation hub"
```

Replace this with a unique value.
7. Click Create and wait for the process to complete.

After you deploy automation hub, you must update the client with the Valid Redirect URIs and Web Origins as described in Updating the Red Hat Single Sign-On client
Additionally, the client comes pre-configured with token mappers, however, if your authentication provider does not provide group data to Red Hat SSO, then the group mapping must be updated to reflect how that information is passed.
This is commonly by user attribute.

## Creating a Keycloak user

This procedure creates a Keycloak user, with the hubadmin role, that can log in to automation hub with Super Administration privileges.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Red Hat Single Sign-On Operator project.
4. Select the Keycloak Realm tab and click Create Keycloak User.
5. On the Keycloak User form, select YAML view.
6. Replace the default YAML file with the following:

```
apiVersion: keycloak.org/v1alpha1
kind: KeycloakUser
metadata:
  name: hubadmin-user
  labels:
    app: sso
    realm: ansible-automation-platform
  namespace: rh-sso
spec:
  realmSelector:
    matchLabels:
      app: sso
      realm: ansible-automation-platform
  user:
    username: hub_admin
    firstName: Hub
    lastName: Admin
    email: hub_admin@example.com
    enabled: true
    emailVerified: false
    credentials:
      - type: password
        value: <ch8ngeme>
    clientRoles:
      automation-hub:
        - hubadmin
```

7. Click Create and wait for the process to complete.

When a user is created, the Operator creates a Secret containing both the username and password using the following naming pattern: credential-<realm name>-<username>-<namespace>.
In this example the credential is called credential-ansible-automation-platform-hub-admin-rh-sso.
When a user is created the Operator does not update the user&#8217;s password.
Password changes are not reflected in the Secret.

## Creating a Red Hat Single Sign-On connection secret

Use this procedure to create a connection secret for Red Hat Single Sign-On.

1. Navigate to \https://<sso_host>/auth/realms/ansible-automation-platform.
2. Copy the public_key value.
3. In the OpenShift Web UI, navigate to .
4. Select the ansible-automation-platform project.
5. Click Create, and select From YAML.
6. Edit the following YAML to create the secret

```
apiVersion: v1
kind: Secret
metadata:
  name: automation-hub-sso                       1
  namespace: ansible-automation-platform
type: Opaque
stringData:
  keycloak_host: "keycloak-rh-sso.apps-crc.testing"
  keycloak_port: "443"
  keycloak_protocol: "https"
  keycloak_realm: "ansible-automation-platform"
  keycloak_admin_role: "hubadmin"
  social_auth_keycloak_key: "automation-hub"
  social_auth_keycloak_secret: "client-secret"   2
  social_auth_keycloak_public_key: >-            3
```

This name is used in the next step when creating the automation hub instance.
If the secret was changed when creating the Keycloak client for automation hub be sure to change this value to match.
Enter the value of the public_key for your Ansible Automation Platform Operator deployment.
7. Click Create and wait for the process to complete.

## Installing automation hub using the Ansible Automation Platform Operator

Use the following procedure to install automation hub using the Ansible Automation Platform Operator.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Automation hub tab.
5. Click Create Automation hub.
6. Select YAML view.
The YAML should be similar to:

```
apiVersion: automationhub.ansible.com/v1beta1
kind: AutomationHub
metadata:
  name: private-ah                              1
  namespace: aap
spec:
  sso_secret: automation-hub-sso                2
  pulp_settings:
    verify_ssl: false
  route_tls_termination_mechanism: Edge
  ingress_type: Route
  loadbalancer_port: 80
  file_storage_size: 100Gi
  image_pull_policy: IfNotPresent
  replicas: 1                                   3
  web_replicas: N
  task_replicas: N
  file_storage_access_mode: ReadWriteMany
  content:
    log_level: INFO
    replicas: 2
  postgres_storage_requirements:
    limits:
      storage: 50Gi
    requests:
      storage: 8Gi
  api:
    log_level: INFO
    replicas: 1
  postgres_resource_requirements:
    limits:
      cpu: 1000m
      memory: 8Gi
    requests:
      cpu: 500m
      memory: 2Gi
  loadbalancer_protocol: http
  resource_manager:
    replicas: 1
  worker:
    replicas: 2
```

Set metadata.name to the name to use for the instance.
Set spec.sso_secret to the name of the secret created in Creating a Secret to hold the Red Hat Single Sign On connection details.
Scale replicas up or down for each deployment by using the web_replicas or task_replicas respectively, where N represents the number of replicas you want to create. Alternatively, you can scale all pods across both deployments by using replicas. See Scaling the Web and Task Pods independently for details.

[NOTE]
----
This YAML turns off SSL verification (ssl_verify: false).
If you are not using self-signed certificates for OpenShift this setting can be removed.
----
7. Click Create and wait for the process to complete.

## Determining the automation hub Route

Use the following procedure to determine the hub route.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the project you used for the install.
4. Copy the location of the private-ah-web-svc service.
The name of the service is different if you used a different name when creating the automation hub instance.
This is used later to update the Red Hat Single Sign-On client.

## Updating the Red Hat Single Sign-On client

After you install automation hub and you know the URL of the instance, you must update the Red Hat Single Sign-On to set the Valid Redirect URIs and Web Origins settings.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the RH-SSO project.
4. Click Red Hat Single Sign-On Operator.
5. Select Keycloak Client.
6. Click on the automation-hub-client-secret client.
7. Select YAML.
8. Update the Client YAML to add the Valid Redirect URIs and Web Origins settings.

```
redirectUris:
  - 'https://private-ah-ansible-automation-platform.apps-crc.testing/*'
webOrigins:
  - 'https://private-ah-ansible-automation-platform.apps-crc.testing'
```



[NOTE]
----
Ensure the indentation is correct when entering these settings.
----
9. Click Save.

1. Navigate to the automation hub route.
2. Enter the hub_admin user credentials and sign in.
3. Red Hat Single Sign-On processes the authentication and redirects back to automation hub.

## Additional resources

* For more information on running operators on OpenShift Container Platform, see Working with Operators in OpenShift Container Platform in the OpenShift Container Platform product documentation.

# Migrating Red Hat Ansible Automation Platform to Red Hat Ansible Automation Platform Operator

Migrating your Red Hat Ansible Automation Platform deployment to the Ansible Automation Platform Operator allows you to take advantage of the benefits provided by a Kubernetes native operator, including simplified upgrades and full lifecycle support for your Red Hat Ansible Automation Platform deployments.


[NOTE]
----
Upgrades of Event-Driven Ansible version 2.4 to 2.5 are not supported. Database migrations between Event-Driven Ansible 2.4 and Event-Driven Ansible 2.5 are not compatible.
----

Use these procedures to migrate any of the following deployments to the Ansible Automation Platform Operator:

* OpenShift cluster A to OpenShift cluster B
* OpenShift namespace A to OpenShift namespace B
* Virtual machine (VM) based or containerized VM Ansible Automation Platform 2.5  Ansible Automation Platform 2.5

## Migration considerations

If you are upgrading from any version of Ansible Automation Platform older than 2.4, you must upgrade through Ansible Automation Platform first.
If you are on OpenShift Container Platform 3 and you want to upgrade to OpenShift Container Platform 4, you must provision a fresh OpenShift Container Platform version 4 cluster and then migrate the Ansible Automation Platform to the new cluster.

## Preparing for migration

Before migrating your current Ansible Automation Platform deployment to Ansible Automation Platform Operator, you must back up your existing data, and create Kubernetes secrets for your secret key and postgresql configuration.


[NOTE]
----
If you are migrating both automation controller and automation hub instances, repeat the steps in Creating a secret key secret and Creating a postgresql configuration secret for both and then proceed to Migrating data to the Ansible Automation Platform Operator.
----

### Migrating to Ansible Automation Platform Operator

To migrate Ansible Automation Platform deployment to Ansible Automation Platform Operator, you must have the following:

* Secret key secret
* Postgresql configuration
* Role-based Access Control for the namespaces on the new OpenShift cluster
* The new OpenShift cluster must be able to connect to the previous PostgreSQL database


[NOTE]
----
You can store the secret key information in the inventory file before the initial Red Hat Ansible Automation Platform installation.
If you are unable to remember your secret key or have trouble locating your inventory file, contact Ansible support through the Red Hat Customer portal.
----

Before migrating your data from Ansible Automation Platform 2.4, you must back up your data for loss prevention.

1. Log in to your current deployment project.
2. Run $ ./setup.sh -b to create a backup of your current data or deployment.

### Creating a secret key secret

To migrate your data to Ansible Automation Platform Operator on OpenShift Container Platform, you must create a secret key that matches the secret key defined in the inventory file during your initial installation.
Otherwise, the migrated data remains encrypted and unusable after migration.

1. Locate the old secret key in the inventory file you used to deploy Ansible Automation Platform in your previous installation.
2. Create a YAML file for your secret key:

```
apiVersion: v1
kind: Secret
metadata:
  name: <resourcename>-secret-key
  namespace: <target-namespace>
stringData:
  secret-key: <content of /etc/tower/SECRET_KEY from old controller>
type: Opaque
```


[NOTE]
----
If admin_password_secret is not provided, the operator looks for a secret named <resourcename>-admin-password for the admin password.
If it is not present, the operator generates a password and create a secret from it named <resourcename>-admin-password.
----
3. Apply the secret key yaml to the cluster:

```
oc apply -f <secret-key.yml>
```


### Creating a postgresql configuration secret

For migration to be successful, you must provide access to the database for your existing deployment.

1. Create a YAML file for your postgresql configuration secret:

```
apiVersion: v1
kind: Secret
metadata:
  name: <resourcename>-old-postgres-configuration
  namespace: <target namespace>
stringData:
  host: "<external ip or url resolvable by the cluster>"
  port: "<external port, this usually defaults to 5432>"
  database: "<desired database name>"
  username: "<username to connect as>"
  password: "<password to connect with>"
type: Opaque
```

2. Apply the postgresql configuration yaml to the cluster:


```
oc apply -f <old-postgres-configuration.yml>
```


### Verifying network connectivity

To ensure successful migration of your data, verify that you have network connectivity from your new operator deployment to your old deployment database.

Take note of the host and port information from your existing deployment. This information is located in the postgres.py file located in the conf.d directory.

1. Create a YAML file to verify the connection between your new deployment and your old deployment database:

```
apiVersion: v1
kind: Pod
metadata:
    name: dbchecker
spec:
  containers:
    - name: dbchecker
      image: registry.redhat.io/rhel8/postgresql-13:latest
      command: ["sleep"]
      args: ["600"]
```

2. Apply the connection checker yaml file to your new project deployment:

```
oc project ansible-automation-platform
oc apply -f connection_checker.yaml
```

3. Verify that the connection checker pod is running:

```
oc get pods
```

4. Connect to a pod shell:

```
oc rsh dbchecker
```

5. After the shell session opens in the pod, verify that the new project can connect to your old project cluster:

```
pg_isready -h <old-host-address> -p <old-port-number> -U awx
```

Example

```
<old-host-address>:<old-port-number> - accepting connections
```


## Migrating data to the Ansible Automation Platform Operator

After you have set your secret key, postgresql credentials, verified network connectivity, and installed the Ansible Automation Platform Operator, you must create a custom resource controller object before you can migrate your data.

### Creating an AutomationController object

Use the following steps to create an AutomationController custom resource object.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Ansible Automation Platform Operator installed on your project namespace.
4. Select the Automation Controller tab.
5. Click Create AutomationController. You can create the object through the Form view or YAML view. The following inputs are available through the Form view.
1. Enter a name for the new deployment.
2. In Advanced configurations:
1. From the Secret Key list, select your secret key secret.
2. From the Old Database Configuration Secret list, select the old postgres configuration secret.
3. Click Create.

### Creating an AutomationHub object

Use the following steps to create an AutomationHub custom resource object.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Ansible Automation Platform Operator installed on your project namespace.
4. Select the Automation Hub tab.
5. Click Create AutomationHub. You can create the object through the Form view or YAML view.
The following inputs are available through the Form view.
1. Enter a name for the new deployment.
2. In Advanced configurations:
1. From the Admin Password Secret list, select your secret key secret.
2. From the Database Configuration Secret list, select the postgres configuration secret.
3. Click Create.

### Creating an EDA object

Use the following steps to create an EDA custom resource object.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select the Ansible Automation Platform Operator installed on your project namespace.
4. Select the Automation Hub tab.
5. Click Create AutomationHub. You can create the object through the Form view or YAML view. The following inputs are available through the Form view.
1. Enter a name for the new deployment.
2. In Advanced configurations:
1. From the Admin Password Secret list, select your secret key secret.
2. From the Database Configuration Secret list, select the postgres configuration secret.
3. Click Create.

## Post migration cleanup

After your data migration is complete, you must delete any Instance Groups that are no longer required.

1. Log in to Red Hat Ansible Automation Platform as the administrator with the password you created during migration.

[NOTE]
----
If you did not create an administrator password during migration, one was automatically created for you.
To locate this password, go to your project, select  and open controller-admin-password.
From there you can copy the password and paste it into the Red Hat Ansible Automation Platform password field.
----
2. Select .
3. Select all Instance Groups except controlplane and default.
4. Click Delete.

# Upgrading Red Hat Ansible Automation Platform Operator on OpenShift Container Platform

The Ansible Automation Platform Operator simplifies the installation, upgrade, and deployment of new Red Hat Ansible Automation Platform instances in your OpenShift Container Platform environment.

## Overview

You can use this document for help with upgrading Ansible Automation Platform 2.4 to 2.5 on Red Hat OpenShift Container Platform.
This document applies to upgrades of Ansible Automation Platform 2.5 to later versions of 2.5.

The Ansible Automation Platform Operator manages deployments, upgrades, backups, and restores of automation controller and automation hub.
It also handles deployments of AnsibleJob and JobTemplate resources from the Ansible Automation Platform Resource Operator.

Each operator version has default automation controller and automation hub versions.
When the operator is upgraded, it also upgrades the automation controller and automation hub deployments it manages, unless overridden in the spec.

OpenShift deployments of Ansible Automation Platform use the built-in Operator Lifecycle Management (OLM) functionality.
For more information, see Operator Lifecycle Manager concepts and resources.
OpenShift does this by using Subscription, CSV, InstallPlan, and OperatorGroup objects.
Most users will not have to interact directly with these resources.
They are created when the Ansible Automation Platform Operator is installed from OperatorHub and managed through the Subscriptions tab in the OpenShift console UI.
For more information, refer to Accessing the web console.



## Upgrade considerations

If you are upgrading from version 2.4, continue to the Upgrading the Ansible Automation Platform Operator.

If your OpenShift Container Platform version is not supported by the Red Hat Ansible Automation Platform version you are upgrading to, you must upgrade your OpenShift Container Platform cluster to a supported version first.

Refer to the Red Hat Ansible Automation Platform Life Cycle to determine the OpenShift Container Platform version needed.

For information about upgrading your cluster, refer to Updating clusters.

## Prerequisites

To upgrade to a newer version of Ansible Automation Platform Operator, you must:

* Create AutomationControllerBackup and AutomationHubBackup objects. For help with this see Backup and recovery for operator environments
* Review the Release notes for the new Ansible Automation Platform version to which you are upgrading and any intermediate versions.
* Determine the type of upgrade you want to perform. See the Channel Upgrades section for more information.

## Channel upgrades

Upgrading to version 2.5 from Ansible Automation Platform 2.4 involves retrieving updates from a channel.
A channel refers to a location where you can access your update.
It currently resides in the OpenShift console UI.



### In-channel upgrades

Most upgrades occur within a channel as follows:

1. A new update becomes available in the marketplace, through the redhat-operator CatalogSource.
2. The system automatically creates a new InstallPlan for your Ansible Automation Platform subscription.
* If set to Manual, the InstallPlan needs manual approval in the OpenShift UI.
* If set to Automatic, it upgrades as soon as the new version is available.

[NOTE]
----
Set a manual install strategy on your Ansible Automation Platform Operator subscription during installation or upgrade. You will be prompted to approve upgrades when available in your chosen update channel. Stable channels, like stable-2.5, are available for each X.Y release.
----
3. A new subscription, CSV, and operator containers are created alongside the old ones.
The old resources are cleaned up after a successful install.

### Cross-channel upgrades

Upgrading between X.Y channels is always manual and intentional.
Stable channels for major and minor versions are in the Operator Catalog.
Currently, only version 2.x is available, so there are few channels.
It is recommended to stay on the latest minor version channel for the latest patches.

If the subscription is set for manual upgrades, you must approve the upgrade in the UI. Then, the system upgrades the Operator to the latest version in that channel.


[NOTE]
----
It is recommended to set a manual install strategy on your Ansible Automation Platform Operator subscription during installation or upgrade.
You will be prompted to approve upgrades when they become available in your chosen update channel.
Stable channels, such as stable-2.5, are available for each X.Y release.
----

The containers provided in the latest channel are updated regularly for OS upgrades and critical fixes. This allows customers to receive critical patches and CVE fixes faster. Larger changes and new features are saved for minor and major releases.

For each major or minor version channel, there is a corresponding "cluster-scoped" channel available. Cluster-scoped channels deploy operators that can manage all namespaces, while non-cluster-scoped channels can only manage resources in their own namespace.


[IMPORTANT]
----
Cluster-scoped bundles are not compatible with namespace-scoped bundles. Do not try to switch between normal (stable-2.4 for example) channels and cluster-scoped (stable-2.4-cluster-scoped) channels, as this is not supported.
----

## Upgrading the Ansible Automation Platform Operator

To upgrade to the latest version of Ansible Automation Platform Operator on OpenShift Container Platform, you can do the following:

* Read the Release notes for 2.5
* [Optional] You need to deploy all of your Red Hat Ansible Automation Platform services (automation controller, automation hub, Event-Driven Ansible) to the same, single namespace before upgrading to 2.5 (only for existing deployments). For more information see, Migrating from one namespace to another.
* Review the Backup and recovery for operator environments guide and backup your services:
* AutomationControllerBackup
* AutomationHubBackup
* EDABackup

1. Log in to OpenShift Container Platform.
2. Navigate to .
3. Select the Ansible Automation Platform Operator installed on your project namespace.
4. Select the Subscriptions tab.
5. Change the channel from stable-2.4 to stable-2.5. An InstallPlan is created for the user.
6. Click Preview InstallPlan.
7. Click Approve.
8. Create a Custom Resource (CR) using the Ansible Automation Platform UI.
The automation controller and automation hub UIs remain until all SSO configuration is supported in the platform gateway UI.

For more information on configuring your updated Ansible Automation Platform Operator, see Configuring the Red Hat Ansible Automation Platform Operator on Red Hat OpenShift Container Platform.

## Creating Ansible Automation Platform custom resources

After upgrading to the latest version of Ansible Automation Platform Operator on OpenShift Container Platform, you can create an Ansible Automation Platform custom resource (CR) that specifies the names of your existing deployments, in the same namespace.

This example outlines the steps to deploy a new Event-Driven Ansible setup after upgrading to the latest version, with existing automation controller and automation hub deployments already in place.

The Appendix contains more examples of Ansible Automation Platform CRs for different deployments.

1. Log in to Red Hat OpenShift Container Platform.
2. Navigate to .
3. Select your Ansible Automation Platform Operator deployment.
4. Select the Details tab.
5. On the Ansible Automation Platform tile click Create instance.
6. From the Create Ansible Automation Platform page enter a name for your instance in the Name field.
7. Click YAML view and paste the following YAML (aap-existing-controller-and-hub-new-eda.yml):

```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller
    disabled: false

  eda:
    disabled: false

  hub:
    name: existing-hub
    disabled: false
```

8. Click Create.


[NOTE]
----
You can override the operators default image for automation controller, automation hub, or platform-resource app images by specifying the preferred image on the YAML spec.
This enables upgrading a specific deployment, like a controller, without updating the operator.
The recommended approach however, is to upgrade the operator and use the default image values.
----

Navigate to your Ansible Automation Platform Operator deployment and click All instances to verify whether all instances have deployed correctly.
You should see the Ansible Automation Platform instance and the deployed AutomationController, EDA, and AutomationHub instances here.

Alternatively, you can verify whether all instances deployed correctly by running oc get route in the command line.

# Adding execution nodes to Red Hat Ansible Automation Platform Operator

You can enable the Ansible Automation Platform Operator with execution nodes by downloading and installing the install bundle.

* An automation controller instance.
* The receptor collection package is installed.
* AAP Repository ansible-automation-platform-2.4-for-rhel-{RHEL-RELEASE-NUMBER}-x86_64-rpms is enabled.

1. Log in to Red Hat Ansible Automation Platform.
2. In the navigation panel, select .
3. Click Add.
4. Input the Execution Node domain name or IP in the Host Name field.
5. Optional: Input the port number in the Listener Port field.
6. Click Save.
7. Click the download icon next to Install Bundle. This starts a download, take note of where you save the file
8. Untar the gz file.

[NOTE]
----
To run the install_receptor.yml playbook you need to install  the receptor collection from Ansible Galaxy:
Ansible-galaxy collection install -r requirements.yml
----
9. Update the playbook with your user name and SSH private key file. Note that ansible_host pre-populates with the hostname you input earlier.

```
all:
   hosts:
      remote-execution:
	        ansible_host: example_host_name # Same with configured in AAP WebUI
	        ansible_user: <username> #user provided
	        Ansible_ssh_private_key_file: ~/.ssh/id_example
```

10. Open your terminal, and navigate to the directory where you saved the playbook.
11. To install the bundle run:

```
ansible-playbook install_receptor.yml -i inventory.yml
```

12. When installed you can now upgrade your execution node by downloading and re-running the playbook for the instance you created.

To verify receptor service status run the following command:


```
sudo systemctl status receptor.service
```


Make sure the service is in active (running) state

To verify if your playbook runs correctly on your new node run the following command:


```
watch podman ps
```


* For more information about managing instance groups see the Managing Instance Groups section of the Automation Controller User Guide.

# Ansible Automation Platform Resource Operator

## Resource Operator overview

Resource Operator is a custom resource (CR) that you can deploy after you have created your platform gateway deployment.
 With Resource Operator you can define projects, job templates, and inventories through the use of YAML files.
 These YAML files are then used by automation controller to create these resources.
 You can create the YAML through the Form view that prompts you for keys and values for your YAML code.
 Alternatively, to work with YAML directly, you can select YAML view.

There are currently two custom resources provided by the Resource Operator:

* AnsibleJob: launches a job in the automation controller instance specified in the Kubernetes secret (automation controller host URL, token).
* JobTemplate: creates a job template in the automation controller instance specified.

## Using Resource Operator

The Resource Operator itself does not do anything until the user creates an object.
As soon as the user creates an AutomationControllerProject or AnsibleJob resource, the Resource Operator will start processing that object.

* Install the Kubernetes-based cluster of your choice.
* Deploy automation controller using the automation-controller-operator.

After installing the automation-controller-resource-operator in your cluster, you must create a Kubernetes (k8s) secret with the connection information for your automation controller instance.
Then you can use Resource Operator to create a k8s resource to manage your automation controller instance.

## Connecting Resource Operator to platform gateway

To connect Resource Operator with platform gateway you need to create a k8s secret with the connection information for your automation controller instance.


[NOTE]
----
You can only create OAuth 2 Tokens for your own user through the API or UI, which means you can only configure or view tokens from your own user profile.
----

To create an OAuth2 token for your user in the platform gateway UI:

1. Log in to Red Hat OpenShift Container Platform.
2. In the navigation panel, select .
3. Select the username you want to create a token for.
4. Select
5. Click Create Token.
6. You can leave Applications empty. Add a description and select Read or Write for the Scope.


[NOTE]
----
Make sure you provide a valid user when creating tokens.
Otherwise, you will get an error message that you tried to issue the command without specifying a user, or supplying a username that does not exist.
----

## Creating a automation controller connection secret for Resource Operator

To make your connection information available to the Resource Operator, create a k8s secret with the token and host value.

1. The following is an example of the YAML for the connection secret.
Save the following example to a file, for example, automation-controller-connection-secret.yml.

```
apiVersion: v1
kind: Secret
metadata:
  name: controller-access
  type: Opaque
stringData:
  token: <generated-token>
  host: https://my-controller-host.example.com/
```

2. Edit the file with your host and token value.
3. Apply it to your cluster by running the kubectl create command:


```
kubectl create -f controller-connection-secret.yml
```


## Creating an AnsibleJob

Launch an automation job on automation controller by creating an AnsibleJob resource.

1. Specify the connection secret and job template you want to launch.

```
apiVersion: tower.ansible.com/v1alpha1
kind: AnsibleJob
metadata:
  generateName: demo-job-1 # generate a unique suffix per 'kubectl create'
spec:
  connection_secret: controller-access
  job_template_name: Demo Job Template
```

2. Configure features such as, inventory, extra variables, and time to live for the job.

```
spec:
  connection_secret: controller-access
  job_template_name: Demo Job Template
  inventory: Demo Inventory                    # Inventory prompt on launch needs to be enabled
  runner_image: quay.io/ansible/controller-resource-runner
  runner_version: latest
  job_ttl: 100
  extra_vars:                                  # Extra variables prompt on launch needs to be enabled
     test_var: test
  job_tags: "provision,install,configuration"  # Specify tags to run
  skip_tags: "configuration,restart"           # Skip tasks with a given tag
```


[NOTE]
----
You must enable  prompt on launch for inventories and extra variables if you are configuring those. To enable Prompt on launch, within the automation controller UI:
From the  page, select your template and select the Prompt on launch checkbox next to Inventory and Variables sections.
----
3. Launch a workflow job template with an AnsibleJob object by specifying the workflow_template_name instead of job_template_name:

```
apiVersion: tower.ansible.com/v1alpha1
kind: AnsibleJob
metadata:
  generateName: demo-job-1 # generate a unique suffix per 'kubectl create'
spec:
  connection_secret: controller-access
  workflow_template_name: Demo Workflow Template
```


## Creating a JobTemplate

* Create a job template on automation controller by creating a JobTemplate resource:

```
apiVersion: tower.ansible.com/v1alpha1
kind: JobTemplate
metadata:
  name: jobtemplate-4
spec:
  connection_secret: controller-access
  job_template_name: ExampleJobTemplate4
  job_template_project: Demo Project
  job_template_playbook: hello_world.yml
  job_template_inventory: Demo Inventory
```


# Appendix: Red Hat Ansible Automation Platform custom resources

This appendix provides a reference for the Ansible Automation Platform custom resources for various deployment scenarios.

## Custom resources

### aap-existing-controller-and-hub-new-eda.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller
    disabled: false

  eda:
    disabled: false

  hub:
    name: existing-hub
    disabled: false
```


### aap-all-defaults.yml


```
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  # Platform
  ## uncomment to test bundle certs
  # bundle_cacert_secret: gateway-custom-certs

  # Components

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

  # lightspeed:
  #   disabled: true

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-existing-controller-only.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller

  eda:
    disabled: true

  hub:
    disabled: true
    ## uncomment if using file storage for Content pod
    # storage_type: file
    # file_storage_storage_class: nfs-local-rwx
    # file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name


# End state:
# * Automation controller: existing-controller registered with Ansible Automation Platform UI
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-existing-hub-and-controller.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller
    disabled: false

  eda:
    disabled: false

  hub:
    name: existing-hub
    disabled: false

# End state:
# * Automation controller: existing-controller registered with Ansible Automation Platform UI
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub: existing-hub registered with Ansible Automation Platform UI
```


### aap-existing-hub-controller-eda.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller # <-- this is the name of the existing AutomationController CR
    disabled: false

  eda:
    name: existing-eda
    disabled: false

  hub:
    name: existing-hub
    disabled: false

# End state:
# * Controller: existing-controller registered with Ansible Automation Platform UI
# * * Event-Driven Ansible: existing-eda registered with Ansible Automation Platform UI
# * * Automation hub: existing-hub registered with Ansible Automation Platform UI
#
# Note: The automation controller, Event-Driven Ansible, and automation hub names must match the names of the existing.
# Automation controller, Event-Driven Ansible, and automation hub CRs in the same namespace as the Ansible Automation Platform CR. If the names do not match, the Ansible Automation Platform CR will not be able to register the existing automation controller, Event-Driven Ansible, and automation hub with the Ansible Automation Platform UI,and will instead deploy new automation controller, Event-Driven Ansible, and automation hub instances.
```


### aap-existing-hub-controller-eda.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    name: existing-controller # <-- this is the name of the existing AutomationController CR
    disabled: false

  eda:
    name: existing-eda
    disabled: false

  hub:
    name: existing-hub
    disabled: false

# End state:
# * Automation controller: existing-controller registered with Ansible Automation Platform UI
# * * Event-Driven Ansible: existing-eda registered with Ansible Automation Platform UI
# * * Automation hub: existing-hub registered with Ansible Automation Platform UI
#
# Note: The automation controller, Event-Driven Ansible, and automation hub names must match the names of the existing.
# Automation controller, Event-Driven Ansible, and automation hub CRs in the same namespace as the Ansible Automation Platform CR. If the names do not match, the Ansible Automation Platform CR will not be able to register the existing automation controller, Event-Driven Ansible, and automation hub with the Ansible Automation Platform UI,and will instead deploy new automation controller, Event-Driven Ansible, and automation hub instances.
```


### aap-fresh-controller-eda.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: false

  eda:
    disabled: false

  hub:
    disabled: true
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

  lightspeed:
    disabled: false

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub disabled
# * Red Hat Ansible Lightspeed disabled
```


### aap-fresh-external-db.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: false

  eda:
    disabled: false

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name


# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-fresh-install-local-management.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  # Platform
  ## uncomment to test bundle certs
  # bundle_cacert_secret: gateway-custom-certs

  # Components
  controller:
    disabled: false
    extra_settings:
      - setting: ALLOW_LOCAL_RESOURCE_MANAGEMENT
        value: 'True'

  eda:
    disabled: false

    extra_settings:
      - setting: EDA_ALLOW_LOCAL_RESOURCE_MANAGEMENT
        value: '@bool True'

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi


    pulp_settings:
      ALLOW_LOCAL_RESOURCE_MANAGEMENT: True

      # cache_enabled: false
      # redirect_to_object_storage: "False"
      # analytics: false
      # galaxy_collection_signing_service: ""
      # galaxy_container_signing_service: ""
      # token_auth_disabled: 'False'
      # token_signature_algorithm: 'ES256'

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

    # Development purposes only
    no_log: false

  # lightspeed:
  #   disabled: true

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-fresh-install-with-settings.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false
  image_pull_policy: Always

  # Platform
  ## uncomment to test bundle certs
  # bundle_cacert_secret: gateway-custom-certs

  # Components
  controller:
    disabled: false
    image_pull_policy: Always

    extra_settings:
      - setting: MAX_PAGE_SIZE
        value: '501'

  eda:
    disabled: false
    image_pull_policy: Always

    extra_settings:
      - setting: EDA_MAX_PAGE_SIZE
        value: '501'

  hub:
    disabled: false
    image_pull_policy: Always

    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: rook-cephfs
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

    pulp_settings:
      MAX_PAGE_SIZE: 501
      cache_enabled: false

  # lightspeed:
  #   disabled: true

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-fresh-install.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  # Redis Mode
  # redis_mode: cluster

  # Platform
  ## uncomment to test bundle certs
  # bundle_cacert_secret: gateway-custom-certs
  # extra_settings:
  #   - setting: MAX_PAGE_SIZE
  #     value: '501'

  # Components
  controller:
    disabled: false

  eda:
    disabled: false

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

  # lightspeed:
  #   disabled: true

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-fresh-only-controller.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: false

  eda:
    disabled: true

  hub:
    disabled: true
    ## uncomment if using file storage for Content pod
    # storage_type: file
    # file_storage_storage_class: nfs-local-rwx
    # file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name


# End state:
# * Automation controller: existing-controller registered with Ansible Automation Platform UI
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
```


### aap-fresh-only-hub.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: true

  eda:
    disabled: true

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    # # AaaS Hub Settings
    # pulp_settings:
    #   cache_enabled: false

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

  lightspeed:
    disabled: false

# End state:
# * Automation controller disabled
# * * Event-Driven Ansible disabled
# * * Automation hub deployed and named: myaap-hub
# * Red Hat Ansible Lightspeed disabled
```


### aap-lightspeed-enabled.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: false

  eda:
    disabled: false

  hub:
    disabled: false
    ## uncomment if using file storage for Content pod
    storage_type: file
    file_storage_storage_class: nfs-local-rwx
    file_storage_size: 10Gi

    ## uncomment if using S3 storage for Content pod
    # storage_type: S3
    # object_storage_s3_secret: example-galaxy-object-storage

    ## uncomment if using Azure storage for Content pod
    # storage_type: azure
    # object_storage_azure_secret: azure-secret-name

  lightspeed:
    disabled: false

# End state:
# * Automation controller deployed and named: myaap-controller
# * * Event-Driven Ansible deployed and named: myaap-eda
# * * Automation hub deployed and named: myaap-hub
# * Red Hat Ansible Lightspeed deployed and named: myaap-lightspeed
```


### gateway-only.yml


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  # Development purposes only
  no_log: false

  controller:
    disabled: true

  eda:
    disabled: true

  hub:
    disabled: true

  lightspeed:
    disabled: true

# End state:
# * Platform gateway deployed and named: myaap-gateway
#   * UI is reachable at: https://myaap-gateway-gateway.apps.ocp4.example.com
# * Automation controller is not deployed
# * * Event-Driven Ansible is not deployed
# * * Automation hub is not deployed
# * Red Hat Ansible Lightspeed is not deployed
```


### EDA_MAX_RUNNING_ACTIVATIONS


```
---
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: myaap
spec:
  eda:
    extra_settings:
      - setting: EDA_MAX_RUNNING_ACTIVATIONS
        value: "15" # Setting this value to "-1" means there will be no limit
```
